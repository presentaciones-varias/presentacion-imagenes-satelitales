---
# title: "Identificación de falseamiento ENUSC"
# author: "Marzo 2024"
format:
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    # footer: <https://quarto.org>
---

#

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 <img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  


[**Imágenes Satelitales**]{.big-par .center-justified}
[***Para* Ciencia de Datos**]{.medium-par.center-justified}

[**Agosto 2024**]{.big-par .center-justified}   


## [Contenidos]{.big-par}  

<!-- ::: {.incremental} -->

- Motivación

- Lenguaje 🔁 Visión Computacional 

- Visión Computacional 🔁 Imágenes Satelitales

- Imágenes satelitales ⏩ INE (Futuro)

- Conclusión



## [Motivación]{.big-par}

::: {.incremental .medium-par}
<!-- - La combinación de imágenes satelitales y machine learning (ML) tiene el potencial para abordar problemas asociados a los objetivos de desarrollo sostenible (ODS) planteados por la ONU en su Agenda 2030: 
  - Estimación de condiciones socioeconómicas y medio-ambientales en una región dada, etc. -->
- INE, a través de encuestas sociales, participa en el **levantamiento de indicadores estadísticos** que miden entre otras cosas el avance del país en los objetivos de desarrollo sostenible (ODS) planteados por Naciones Unidas en su Agenda 2030.
  - Podemos aprovechar el potencial que tiene la **combinación de imágenes satelitales y machine learning (ML)** para abordar problemas asociados al cálculo de estos indicadores:
    - Por ejemplo: estimación de condiciones socioeconómicas y medio-ambientales en una región dada.
:::

:::: {.columns}

::: {.column width="55%"}
[<img src="imagenes/data/sdgs_un.png" width="90%"/>]{.center} 
:::

::: {.column width="35%"}
[<img src="imagenes/data/data_surveys_02.png" width="100%"/>]{.center} 
:::

::::



## [Motivación]{.big-par}

::: {.incremental .medium-par}
- Sin embargo, hacer uso de machine learning + imágenes satelitales para calcular indicadores estadísticos presenta algunos **desafíos**:
  - El volumen existente de **datos satelitales etiquetados es mucho menor que de datos no-etiquetados**. Y esta tendencia se agudiza en el tiempo.
  [<img src="imagenes/papers/Zhu_etal_2024_fig_02a.png" width="50%"/>]{.center} 
  - En general, por diseño, **resultados de encuestas son limitados geográfica y temporalmente**.
  - Existe **limitada infraestructura y capacidad técnica** para que analistas entrenen modelos ML que usen datos de imágenes satelitales, personalizados para cada tarea a resolver.
:::



## [Motivación]{.big-par}

::: {.medium-par}
- Sin embargo, hacer uso de machine learning + imágenes satelitales para calcular indicadores estadísticos presenta algunos **desafíos**:
  - Cómo podemos superar estos desafíos?
:::

. . .

::: {.center}

![](imagenes/data/data_attention_01.png){width="50%"}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.incremental .medium-par}
- Soluciones basadas en *autoregressive language modeling* en GPT y ***masked autoencoding*** en BERT: remueven una porción de los datos (texto) y aprenden a predecir el contenido removido.
  - Estos métodos han permitido entrenar hoy modelos NLP generalizables con 100M+ parámetros.
:::

. . .

::: {.center}

![](imagenes/papers/Radford_etal_2018_fig_01.png){width="60%"}
[(Radford et al., 2018. GPT-1)]{.center .small-par}

![](imagenes/papers/Devlin_etal_2019_fig_01.png){width="60%"}
[(Devlin et al., 2019. BERT)]{.center .small-par}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.incremental .medium-par}
- **Qué diferencia al lenguage de la visión computacional?**
  - En visión, CNNs han dominado en las últimas décadas, pero el **gap de arquitectura se ha cerrado con la introducción de *Vision Transformers (ViT)*** (Dosovitskiy et al.,2021).
  - ViT: Modelo cercanamente basado en el Transformer original (Vaswani et al., 2017).
    - Imagen es dividida en segmentos de tamaño fijo, que se asocian a una secuencia de embeddings posicionales que es pasada al Transformer.
    - Segmentos son tratados como tokens (palabras) en NLP.
    - Entrenamiento en grandes datasets  (14M-300M imágenes) supera a resultados obtenidos previamente con modelos basados en CNNs.
:::

:::: {.columns}

::: {.column width="45%"}
[<img src="imagenes/papers/Dosovitskiy_etal_2021_title.png" width="90%"/>]{.center} 
:::

::: {.column width="45%"}
[<img src="imagenes/papers/Dosovitskiy_etal_2021_citing.png" width="100%"/>]{.center} 
:::

::::

::: {.center}

![](imagenes/papers/Dosovitskiy_etal_2021_fig_01.png){width="70%"}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.incremental .medium-par}
- **Qué diferencia al lenguage de la visión computacional?**
  - Densidad de información: entrenar un modelo para predecir sólo pocas palabras faltantes parece inducir un entendimiento sofisticado del lenguaje (?).
  - En cambio, **imágenes tienen alta redundancia espacial**: segmentos faltantes pueden reconstruirse desde segmentos vecinos sin necesidad de entendimiento global.
  - Solución: **enmascarar una gran porción de segmentos aleatorios, forzando entendimiento holístico de la imagen.**
:::

. . .

::: {.center}

![](imagenes/papers/He_etal_2022_title.png){width="50%"}

![](imagenes/papers/He_etal_2022_citing.png){width="60%"}

:::


## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.incremental .medium-par}
- Así surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual:
  - Encoder + decoder. 
:::

. . .

::: {.center}

![](imagenes/papers/mae/mae_00.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.medium-par}
- Así surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 1. Se divide input data en segmentos (*patches*), análogos a a tokens en NLP.
:::

::: {.center}

![](imagenes/papers/mae/mae_01.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.medium-par}
- Así surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 2. Segmentos se projectan linealmente a un espacio de embeddings, removiendo aleatoriamente una porción significativa de tokens (ej: 75%).
:::

::: {.center}

![](imagenes/papers/mae/mae_02.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.medium-par}
- Así surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 3. Tokens no removidos son pasados a través del encoder.
  - **Encoder es un ViT, pero aplicado sólo a tokens no enmascarados.** 
:::

::: {.center}

![](imagenes/papers/mae/mae_03.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.medium-par}
- Así surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 4. Token removidos son enmascarados, para indicar al modelo qué fue removido del input.
:::

::: {.center}

![](imagenes/papers/mae/mae_04.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.medium-par}
- Así surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 5. Conjunto completo de tokens es pasado a través del decoder, que intenta reconstruir el input.
:::

::: {.center}

![](imagenes/papers/mae/mae_05.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.medium-par}
- Así surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- **En este proceso se entrena encoder y decoder, pero sólo se mantiene el encoder para hacer inferencias en tareas *downstream*.**
:::

::: {.center}

![](imagenes/papers/mae/mae_06.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::




## [Lenguaje 🔁 Visión Computacional]{.big-par}

::: {.incremental .medium-par}
- Ejemplos en datasets ImageNet y COCO.
  - Imagen enmascarada (izquierda), reconstrucción con MAE (centro) y original (derecha).
:::

. . .

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/He_etal_2022_fig_02.png){.fragment .fade-in-then-out width="80%"}

![](imagenes/papers/He_etal_2022_fig_03.png){.fragment .fade-in-then-out width="80%"}

:::

[(He et al., 2022)]{.center .small-par}



## [Visión Computacional 🔁 Imágenes Satelitales]{.big-par}

::: {.incremental .medium-par}
- **P**retrained **R**emote **S**ensing **T**ransf**o**rmer: **Presto**.
:::

. . .

::: {.center}

![](imagenes/papers/papers_arrow/papers_arrow.png){width="90%"}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Modelo basado en *self-supervised masked autoencoding*: encoder-decoder (He et al., 2022).
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_00.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Permite ingestar series de tiempo nivel de pixel desde múltiple productos y sensores de percepción remota.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_01.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::


## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Series de tiempo son enmascaradas aleatoriamente y modelo es entrenado para reconstruir las series originales.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_02.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Encoder (f) embebe la parte del input no enmascarada.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_03.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Decoder (g) intenta reconstruir la parte enmascarada.
:::

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/presto_mae/presto_mae_04.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_fig_02.png){.fragment .fade-in-then-out width="80%"}

:::

[(Tseng et al., 2024)]{.center .small-par}



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- Una vez entrenado, sólo se usa output del encoder.
  - **Encoder puede usarse como *feature extractor* o en *fine-tuning*.**
  - Las representaciones aprendidas pueden ser adaptadas a una amplia variedad de tareas, con **diferentes sensores como input y con diverso número de *timesteps***.
  <!-- - Predicciones son robustas a *missing data*... -->
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Datos de entrenamiento:** dataset representativo globalmente (21.5M pixel samples, resolución de 10m por pixel).
  - Series de tiempo a nivel de pixel para 12 meses (sampleados desde 2 años en 2020-2021); 1 timestep por mes.
  - Múltiples set exportados desde Google Earth Engine.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_fig_06.png){width="60%"}
![](imagenes/papers/Tseng_etal_2024_datasets.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Encoding y tokenización:**
  - **Series de tiempo son transformadas en tokens, c/u representado por un embedding.**
    - Por cada timestamp, se agrupan variables del input según tipo de sensor, resolución espacial y parte del espectro electromagnético (ej: bandas S1 forman un grupo).
    - Cada token es una transformación lineal de un grupo de canales del input. 
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_channel_groups.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Encoding y tokenización:**
  - Se quiere comunicar al modelo características propias de imágenes satelitales: **localización de pixeles, timestamp, grupo de canales**.
  - Se agregan encodings a los embeddings, que contienen una concatenación de:
    - la ubicación de cada pixel en la imagen,
    - timestamp (mes),
    - grupo de canales de cada variable.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_encodings.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Enmascaramiento:** clave para la performance de Presto en inputs incompletos (timesteps y/o canales faltantes).
  - Al enmascarar **se incentiva que el modelo aprenda representaciones que funcionen con subconjuntos de bandas o timesteps.**
  - En cada instancia de entrenamiento, se usó un sampleo aleatorio basado en múltiples estrategias de enmascaramiento:
    - Aleatorio (1), canales-grupos (2), timesteps contiguos (3), timesteps no contiguos (4).
:::

<!-- . . . -->

::: {.center}
![](imagenes/papers/Tseng_etal_2024_masking_techniques.png){width="50%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Experimentos:** a partir del encoder-decoder entrenado, **se descartó el decoder y se pasó el output del encoder a un modelo downstream.**
  - Se evaluaron diferentes modelos para: 
    - **Feature extraction:** regresión lineal o logística (PrestoR), random forest (PrestoRF).
    - **Fine-tuning:** PrestoFT.
  - En diferentes tareas, basadas en **series de tiempo, imágenes, imágenes + series de tiempo**.
:::

. . .

::: {.center}
![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_00.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en series de tiempo**:
  - **Crop type segmentation:** clasificación de pixel binaria de: <br> maíz (Kenya), café (Brazil), terreno de cultivo (Togo).
:::

<!-- . . . -->

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_01.png){.fragment .fade-in-then-out width="65%"}

![](imagenes/papers/Tseng_etal_2024_table_02.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_fig_03.png){.fragment .fade-in-then-out width="80%"}

:::

[(Tseng et al., 2024)]{.center .small-par}


## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en series de tiempo**: regresión
  - **Fuel moisture:** nivel de humedad del combustible vivo (contenido de agua en vegetación) en EEUU (Oeste).
  - **Algae blooms:** severidad de florecimiento de cianobacterias (algas verde-azuladas) en diferentes partes de EEUU.
:::

<!-- . . . -->

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_02.png){.fragment .fade-in-then-out width="65%"}

![](imagenes/papers/Tseng_etal_2024_table_03.png){.fragment .fade-in-then-out width="70%"}

:::

[(Tseng et al., 2024)]{.center .small-par}



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en imágenes**:
  - **TreeSatAI:** The TreeSatAI dataset consists of detecting the presence of one or more tree species (out of 20 possible species) in forestry images in Germany.
  - **EuroSAT:** The EuroSAT dataset classifies Sentinel-2 multispectral images in Europe with one of 10 landcover classes.
:::

<!-- . . . -->

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_03.png){.fragment .fade-in-then-out width="65%"}

![](imagenes/papers/Tseng_etal_2024_fig_04.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_table_04.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_fig_05.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_table_05.png){.fragment .fade-in-then-out width="70%"}

:::

[(Tseng et al., 2024)]{.center .small-par}



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.medium-par}
- **Reproducibilidad**
  - [Github repo](https://github.com/nasaharvest/presto) + [Jupyter notebook](https://github.com/nasaharvest/presto/blob/main/downstream_task_demo.ipynb): ejemplo de cómo aplicar Presto a nueva tarea.
  - [Presentación youtube](https://www.youtube.com/watch?v=H2mHNaI3oNQ)
:::

::: {.center}
![](imagenes/papers/presto_github_01.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Alternativas]{.big-par}

Presto no es el único **Modelos Fundacional** propuestos para imágenes satelitales:

::: {.center}
![](imagenes/data/nasa_foundation_models.png){.fragment .fade-in-then-out width="85%"}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Alternativas]{.big-par}

Presto no es el único **Modelos Fundacional** propuestos para imágenes satelitales:

::: {.center}
![](imagenes/papers/Rolf_etal_2021_fig_01.png){width="100%"}
[(A generalizable and accessible approach to machine learning with global satellite imagery. Rolf et al., 2021)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Alternativas]{.big-par}

Presto no es el único **Modelos Fundacional** propuestos para imágenes satelitales:

::: {.center}
![](imagenes/papers/Rolf_etal_2021_fig_01.png){width="100%"}
[(A generalizable and accessible approach to machine learning with global satellite imagery. Rolf et al., 2021)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Alternativas]{.big-par}

::: {.center}
![](imagenes/papers/Jakubik_etal_2023_news.png){width="70%"}
![](imagenes/papers/Jakubik_etal_2023_fig_01.png){width="70%"}
![](imagenes/papers/Jakubik_etal_2023_fig_03.png){width="70%"}
[(Foundation Models for Generalist Geospatial Artificial Intelligence. Jakubik et al., 2023)]{.center .small-par}
:::



## [Imágenes satelitales ⏩ INE (Futuro)]{.big-par}

::: {.r-stack fragment-index=2 .center}

![](imagenes/workflow/workflow_00.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01b.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01c.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_02a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_03a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_04.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_05.png){.fragment .fade-in-then-out width="90%"}

:::



## [Conclusión]{.big-par}

::: {.incremental .medium-par}
- Podemos aprovechar el potencial que tiene la **combinación de imágenes satelitales y machine learning (ML)** para enriquecer el cálculo de indicadores estadísticos, superando los **desafíos** planteados:
  - *El volumen existente de **datos satelitales etiquetados es mucho menor que de datos no-etiquetados**.*
    - Podemos utilizar modelos fundacionales (FM) pre-entrenados (Presto, etc), ya sea para fine-tuning o feature extraction.
    - Estos modelos capturan el comportamiento emergente del sistema modelado a partir de datos no-etiquetados y tienden a escalar mejor para tareas downstream que modelos específicos entrenados con datos etiquetados.
  - *En general, por diseño, **resultados de encuestas son limitados geográfica y temporalmente**.*
    - Predicciones obtenidas desde el fine-tuning o feature extraction de FMs pueden generalizar estos resultados, especialmente a regiones o intervalos de tiempo en que son escasos.
  - *Existe **limitada infraestructura y capacidad técnica** para que analistas entrenen modelos ML que usen datos de imágenes satelitales, personalizados para cada tarea a resolver.*
    - El fine-tuning o feature extraction de FMs permite entrenar pequeños modelos, con datos etiquetados relevantes para cada tarea.
    - Este entrenamiento no requiere una gran infraestructura ni datos satelitales, por lo que eventualmente podría ser realizado por analistas.
      - Ejemplo: calcular predicciones de algún indicador a un dado nivel geográfico, que sirva de insumo para modelos basados en *Small Are Estimation* (SAE).
:::



## [Extra]{.big-par}



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en imágenes + series de tiempo**:
  - **S2-Agri100:** El dataset S2-Agri clasifica tipos de cultivo crop en parcelas agrícolas.
  - Consiste de 24 timesteps en intervalos de 10-30 días (comparado con Presto, que fue entrenado en series de tiempo de 12 meses).
:::

. . .

::: {.center}
![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_04.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visión Computacional 🔁 Imágenes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en imágenes + series de tiempo**:
  - **S2-Agri100:** El dataset S2-Agri clasifica tipos de cultivo crop en parcelas agrícolas. 
  - Presto logra resultados comparables a modelo SITS-Former, a pesar de tener 6x menos parámetros.
    - Esto demuestra que Presto puede recibir series de tiempo con variada resolución y a diferentes intervalos temporales.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_table_06.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Imágenes satelitales ⏩ INE (Futuro)]{.big-par}

```{mermaid}
flowchart TD
    L11[Dagster: GEE] & L12[Etiquetas tarea: SAE, MMV, etc] --> L2[Presto: encoder]
    L2 --> L3[Dependencia local o regional?]
    L3 -- local ---> L41[Fine Tuning]
    L41 --> L51[Ej: SAE]
    L3 -- regional ---> L42[Feature extraction]
    L42 --> L52[Ej: MMV]
```


#

[<img src="imagenes/logo_portada2.png" width="20%"/>]{.center}

[**Imágenes Satelitales**]{.big-par .center-justified}
[***Para* Ciencia de Datos**]{.medium-par.center-justified}

[**Agosto 2024**]{.big-par .center-justified}


[]{.linea-superior} 
[]{.linea-inferior} 



