---
# title: "Identificaci贸n de falseamiento ENUSC"
# author: "Marzo 2024"
format:
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    # footer: <https://quarto.org>
---

#

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 <img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  


[**Im谩genes Satelitales**]{.big-par .center-justified}
[**Para Ciencia de Datos**]{.medium-par.center-justified}

[**Agosto 2024**]{.big-par .center-justified}   


## [Contenidos]{.big-par}  

<!-- ::: {.incremental} -->

- Motivaci贸n

- Lenguaje  Visi贸n Computacional 

- Visi贸n Computacional  Im谩genes Satelitales

- Im谩genes satelitales  INE (Futuro)

- Conclusi贸n



## [Motivaci贸n]{.big-par}

::: {.incremental .medium-par}
- La combinaci贸n de im谩genes satelitales y machine learning (ML) tiene el potencial para abordar desaf铆os globales como la estimaci贸n de condiciones socioecon贸micas y medio-ambientales en una regi贸n dada.
- Como equipo, tenemos la oportunidad / responsabilidad de apoyar en la generaci贸n de estad铆sticas basadas en resultados de encuestas INE...
  - En muchos casos estos resultados est谩n restringidos en el espacio y tiempo...
  - Haciendo uso de im谩genes satelitales y ML, podemos intentar generalizar estos resultados, especialmente en regiones en que son escasos.
  - Ejemplo de aplicaci贸n: potenciar modelos basados en *Small Are Estimation* (SAE) con observaciones de percepci贸n remota.
- Existe limitada infraestructura y capacidad t茅cnica para que analistas entrenen m煤ltiples modelos ML con datos de im谩genes satelitales, que se ajusten a cada tarea a resolver.
  - Esto puede ser mejorado dando la posibilidad de entrenar peque帽os modelos que escalen eficientemente en sus predicciones... 
:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.incremental .medium-par}
- Soluciones basadas en *autoregressive language modeling* en GPT y *masked autoencoding* en BERT: remueven una porci贸n de los datos (texto) y aprenden a predecir el contenido removido.
  - Estos m茅todos han permitido entrenar hoy modelos NLP generalizables con 100M+ par谩metros.
:::

. . .

::: {.center}

![](imagenes/papers/Radford_etal_2018_fig_01.png){width="60%"}
[(Radford et al., 2018. GPT-1)]{.center .small-par}

![](imagenes/papers/Devlin_etal_2019_fig_01.png){width="60%"}
[(Devlin et al., 2019. BERT)]{.center .small-par}

:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.incremental .medium-par}
- **Qu茅 diferencia al lenguage de la visi贸n computacional?**
  - En visi贸n, CNNs han dominado en las 煤ltimas d茅cadas, pero el gap de arquitectura se ha cerrado con la introducci贸n de *Vision Transformers (ViT)* (Dosovitskiy et al.,2021).
  - ViT: Modelo cercanamente basado en el Transformer original (Vaswani et al., 2017).
    - Imagen es dividida en segmentos de tama帽o fijo, que se asocian a una secuencia de embeddings posicionales que es pasada al Transformer.
    - Segmentos son tratados como tokens (palabras) en NLP.
    - Entrenamiento en grandes datasets  (14M-300M im谩genes) supera a resultados obtenidos previamente con modelos basados en CNNs.
:::

. . .

:::: {layout-ncol=2}

::: fragment

[<img src="imagenes/papers/Dosovitskiy_etal_2021_title.png" width="90%"/>]{.center} 

:::

::: fragment

[<img src="imagenes/papers/Dosovitskiy_etal_2021_citing.png" width="90%"/>]{.center} 

:::

::::

. . .

::: {.center}

![](imagenes/papers/Dosovitskiy_etal_2021_fig_01.png){width="70%"}

:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.incremental .medium-par}
- **Qu茅 diferencia al lenguage de la visi贸n computacional?**
  - Densidad de informaci贸n: entrenar un modelo para predecir s贸lo pocas palabras faltantes parece inducir un entendimiento sofisticado del lenguaje (?).
  - En cambio, im谩genes tienen alta redundancia espacial: segmentos faltantes pueden reconstruirse desde segmentos vecinos sin necesidad de entendimiento global.
  - Soluci贸n: **enmascarar una gran porci贸n de segmentos aleatorios, forzando entendimiento hol铆stico de la imagen.**
:::

. . .

::: {.center}

![](imagenes/papers/He_etal_2022_title.png){width="50%"}

![](imagenes/papers/He_etal_2022_citing.png){width="60%"}

:::


## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.incremental .medium-par}
- As铆 surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual:
  - Encoder + decoder. 
:::

. . .

::: {.center}

![](imagenes/papers/mae/mae_00.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.medium-par}
- As铆 surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 1. Se divide input data en segmentos (*patches*), an谩logos a a tokens en NLP.
:::

::: {.center}

![](imagenes/papers/mae/mae_01.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.medium-par}
- As铆 surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 2. Segmentos se projectan linealmente a un espacio de embeddings, removiendo aleatoriamente una porci贸n significativa de tokens (ej: 75%).
:::

::: {.center}

![](imagenes/papers/mae/mae_02.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.medium-par}
- As铆 surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 3. Tokens no removidos son pasados a trav茅s del encoder.
  - Encoder es un ViT, pero aplicado s贸lo a tokens no enmascarados. 
:::

::: {.center}

![](imagenes/papers/mae/mae_03.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.medium-par}
- As铆 surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 4. Token removidos son enmascarados, para indicar al modelo qu茅 fue removido del input.
:::

::: {.center}

![](imagenes/papers/mae/mae_04.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.medium-par}
- As铆 surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 5. Conjunto completo de tokens es pasado a trav茅s del decoder, que intenta reconstruir el input.
:::

::: {.center}

![](imagenes/papers/mae/mae_05.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.medium-par}
- As铆 surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- En este proceso se entrena encoder y decoder, pero s贸lo se mantiene el encoder para hacer inferencias en tareas *downstream*.
:::

::: {.center}

![](imagenes/papers/mae/mae_06.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::




## [Lenguaje  Visi贸n Computacional]{.big-par}

::: {.incremental .medium-par}
- Ejemplos en datasets ImageNet y COCO.
  - Imagen enmascarada (izquierda), reconstrucci贸n con MAE (centro) y original (derecha).
:::

. . .

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/He_etal_2022_fig_02.png){.fragment .fade-in-then-out width="80%"}

![](imagenes/papers/He_etal_2022_fig_03.png){.fragment .fade-in-then-out width="80%"}

:::

[(He et al., 2022)]{.center .small-par}



## [Visi贸n Computacional  Im谩genes Satelitales]{.big-par}

::: {.incremental .medium-par}
- **P**retrained **R**emote **S**ensing **T**ransf**o**rmer: **Presto**.
:::

. . .

::: {.center}

![](imagenes/papers/papers_arrow/papers_arrow.png){width="90%"}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Modelo basado en *self-supervised masked autoencoding*: encoder-decoder (He et al., 2022).
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_00.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Permite ingestar series de tiempo nivel de pixel desde m煤ltiple productos y sensores de percepci贸n remota.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_01.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::


## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Series de tiempo son enmascaradas aleatoriamente y modelo es entrenado para reconstruir las series originales.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_02.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Encoder (f) embebe la parte del input no enmascarada.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_03.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Decoder (g) intenta reconstruir la parte enmascarada.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_04.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

<!-- ::: {.incremental .medium-par}
- **Presto:** ...
::: -->

<!-- . . . -->

::: {.center}
![](imagenes/papers/Tseng_etal_2024_fig_02.png){width="80%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- Una vez entrenado, s贸lo se usa output del encoder.
  - Encoder puede usarse como *feature extractor* o en *fine-tuning*.
  - Las representaciones aprendidas pueden ser adaptadas a una amplia variedad de tareas, con diferentes sensores como input y con diverso n煤mero de *timesteps*.
  - Predicciones son robustas a *missing data*...
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Datos de entrenamiento:** dataset representativo globalmente (21.5M pixel samples, resoluci贸n de 10m por pixel).
  - Series de tiempo a nivel de pixel para 12 meses (sampleados desde 2 a帽os en 2020-2021); 1 timestep por mes.
  - M煤ltiples set exportados desde Google Earth Engine.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_fig_06.png){width="60%"}
![](imagenes/papers/Tseng_etal_2024_datasets.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Encoding y tokenizaci贸n:**
  - Series de tiempo son transformadas en tokens, c/u representado por un embedding.
    - Por cada timestamp, se agrupan variables del input seg煤n tipo de sensor, resoluci贸n espacial y parte del espectro electromagn茅tico (ej: bandas S1 forman un grupo).
    - Cada token es una transformaci贸n lineal de un grupo de canales del input. 
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_channel_groups.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Encoding y tokenizaci贸n:**
  - Se quiere comunicar al modelo caracter铆sticas propias de im谩genes satelitales: **localizaci贸n de pixeles, timestamp, grupo de canales**.
  - Se agregan encodings a los embeddings, que contienen una concatenaci贸n de:
    - la ubicaci贸n de cada pixel en la imagen,
    - timestamp (mes),
    - grupo de canales de cada variable.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_encodings.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Enmascaramiento:** clave para la performance de Presto en inputs incompletos (timesteps y/o canales faltantes).
  - Al enmascarar se incentiva que el modelo aprenda representaciones que funcionen con subconjuntos de bandas o timesteps.
  - En cada instancia de entrenamiento, se us贸 un sampleo aleatorio basado en m煤ltiples estrategias de enmascaramiento:
    - Aleatorio (1), canales-grupos (2), timesteps contiguos (3), timesteps no contiguos (4).
:::

<!-- . . . -->

::: {.center}
![](imagenes/papers/Tseng_etal_2024_masking_techniques.png){width="50%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Experimentos:** a partir del encoder-decoder entrenado, se descart贸 el decoder y se pas贸 el output del encoder a un modelo downstream.
  - Se evaluaron diferentes modelos para: 
    - **Feature extraction:** regresi贸n lineal o log铆stica (PrestoR), random forest (PrestoRF).
    - **Fine-tuning:** PrestoFT.
  - En diferentes tareas, basadas en **series de tiempo, im谩genes, im谩genes + series de tiempo**.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_table_01.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en series de tiempo**:
  - **Crop type segmentation:** clasificaci贸n de pixel binaria de: ma铆z (Kenya), caf茅 (Brazil), *cropland* (Togo).
  - **Fuel moisture:** regresi贸n para calcular *live fuel moisture content in Western U.S.*
  - **Florecimiento de algas:** mide severidad de florecimiento cyanobacterial algal en diferentes partes de U.S.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_fig_03.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en en im谩genes**:
  - **TreeSatAI:** The TreeSatAI dataset consists of detecting the presence of one or more tree species (out of 20 possible species) in forestry images in Germany.
  - **EuroSAT:** The EuroSAT dataset classifies Sentinel-2 multispectral images in Europe with one of 10 landcover classes.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_fig_04.png){width="60%"}
![](imagenes/papers/Tseng_etal_2024_fig_05.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en en im谩genes + series de tiempo**:
  - **S2-Agri100:** The S2-Agri dataset classifies crop types in agricultural parcels.
  - This dataset consists of 24 timesteps at 10 to 30 day intervals (compared to Prestos pre-training data, which consists of 12-month timeseries). 
    - Presto remained performant on this dataset, achieving comparable results with SITS-Former despite having 6X fewer parameters. 
    - This shows that Presto can ingest timeseries at different temporal resolutions and at varying intervals.
:::



## [Visi贸n Computacional  Im谩genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- **Presto: Reproducibilidad**
- [Github repo](https://github.com/nasaharvest/presto) + [Jupyter notebook](https://github.com/nasaharvest/presto/blob/main/downstream_task_demo.ipynb): ejemplo de c贸mo aplicar Presto a nueva tarea.
- [Presentaci贸n youtube](https://www.youtube.com/watch?v=H2mHNaI3oNQ)
:::

::: {.center}
![](imagenes/papers/presto_github_01.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [Im谩genes satelitales | Esquema futuro posible]{.big-par}

::: {.medium-par}
- TODO: REMOVE THIS SLIDE
:::


```{mermaid}
flowchart TD
    L11[Dagster: GEE] & L12[Etiquetas tarea: SAE, MMV, etc] --> L2[Presto: encoder]
    L2 --> L3[Dependencia local o regional?]
    L3 -- local ---> L41[Fine Tuning]
    L41 --> L51[Ej: SAE]
    L3 -- regional ---> L42[Feature extraction]
    L42 --> L52[Ej: MMV]
```



## [Im谩genes satelitales  INE (Futuro)]{.big-par}

::: {.r-stack fragment-index=2 .center}

![](imagenes/workflow/workflow_00.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01b.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01c.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_02a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_03a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_04.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_05.png){.fragment .fade-in-then-out width="90%"}

:::



## [Conclusi贸n]{.big-par}

::: {.incremental .medium-par}
- ...
:::



#

[<img src="imagenes/logo_portada2.png" width="20%"/>]{.center}

[**Im谩genes Satelitales**]{.big-par .center-justified}
[**Para Ciencia de Datos**]{.medium-par.center-justified}

[**Agosto 2024**]{.big-par .center-justified}


[]{.linea-superior} 
[]{.linea-inferior} 



