---
# title: "IdentificaciÃ³n de falseamiento ENUSC"
# author: "Marzo 2024"
format:
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    # footer: <https://quarto.org>
---

#

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 <img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  


[**ImÃ¡genes Satelitales**]{.big-par .center-justified}
[***Para* Ciencia de Datos**]{.medium-par.center-justified}

[**Agosto 2024**]{.big-par .center-justified}   


## [Contenidos]{.big-par}  

<!-- ::: {.incremental} -->

- MotivaciÃ³n

- Lenguaje ğŸ” VisiÃ³n Computacional 

- VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales

- ImÃ¡genes satelitales â© INE (Futuro)

- ConclusiÃ³n



## [MotivaciÃ³n]{.big-par}

::: {.incremental .medium-par}
<!-- - La combinaciÃ³n de imÃ¡genes satelitales y machine learning (ML) tiene el potencial para abordar problemas asociados a los objetivos de desarrollo sostenible (ODS) planteados por la ONU en su Agenda 2030: 
  - EstimaciÃ³n de condiciones socioeconÃ³micas y medio-ambientales en una regiÃ³n dada, etc. -->
- INE, a travÃ©s de encuestas sociales, participa en el **levantamiento de indicadores estadÃ­sticos** que miden entre otras cosas el avance del paÃ­s en los objetivos de desarrollo sostenible (ODS) planteados por Naciones Unidas en su Agenda 2030.
  - Podemos aprovechar el potencial que tiene la **combinaciÃ³n de imÃ¡genes satelitales y machine learning (ML)** para abordar problemas asociados al cÃ¡lculo de estos indicadores:
    - Por ejemplo: estimaciÃ³n de condiciones socioeconÃ³micas y medio-ambientales en una regiÃ³n dada.
:::

:::: {.columns}

::: {.column width="55%"}
[<img src="imagenes/data/sdgs_un.png" width="90%"/>]{.center} 
:::

::: {.column width="35%"}
[<img src="imagenes/data/data_surveys_02.png" width="100%"/>]{.center} 
:::

::::



## [MotivaciÃ³n]{.big-par}

::: {.incremental .medium-par}
- Sin embargo, hacer uso de machine learning + imÃ¡genes satelitales para calcular indicadores estadÃ­sticos presenta algunos **desafÃ­os**:
  - El volumen existente de **datos satelitales etiquetados es mucho menor que de datos no-etiquetados**. Y esta tendencia se agudiza en el tiempo.
  [<img src="imagenes/papers/Zhu_etal_2024_fig_02a.png" width="50%"/>]{.center} 
  - En general, por diseÃ±o, **resultados de encuestas son limitados geogrÃ¡fica y temporalmente**.
  - Existe **limitada infraestructura y capacidad tÃ©cnica** para que analistas entrenen modelos ML que usen datos de imÃ¡genes satelitales, personalizados para cada tarea a resolver.
:::



## [MotivaciÃ³n]{.big-par}

::: {.medium-par}
- Sin embargo, hacer uso de machine learning + imÃ¡genes satelitales para calcular indicadores estadÃ­sticos presenta algunos **desafÃ­os**:
  - CÃ³mo podemos superar estos desafÃ­os?
:::

. . .

::: {.center}

![](imagenes/data/data_attention_01.png){width="50%"}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.incremental .medium-par}
- Soluciones basadas en *autoregressive language modeling* en GPT y ***masked autoencoding*** en BERT: remueven una porciÃ³n de los datos (texto) y aprenden a predecir el contenido removido.
  - Estos mÃ©todos han permitido entrenar hoy modelos NLP generalizables con 100M+ parÃ¡metros.
:::

. . .

::: {.center}

![](imagenes/papers/Radford_etal_2018_fig_01.png){width="60%"}
[(Radford et al., 2018. GPT-1)]{.center .small-par}

![](imagenes/papers/Devlin_etal_2019_fig_01.png){width="60%"}
[(Devlin et al., 2019. BERT)]{.center .small-par}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.incremental .medium-par}
- **QuÃ© diferencia al lenguage de la visiÃ³n computacional?**
  - En visiÃ³n, CNNs han dominado en las Ãºltimas dÃ©cadas, pero el **gap de arquitectura se ha cerrado con la introducciÃ³n de *Vision Transformers (ViT)*** (Dosovitskiy et al.,2021).
  - ViT: Modelo cercanamente basado en el Transformer original (Vaswani et al., 2017).
    - Imagen es dividida en segmentos de tamaÃ±o fijo, que se asocian a una secuencia de embeddings posicionales que es pasada al Transformer.
    - Segmentos son tratados como tokens (palabras) en NLP.
    - Entrenamiento en grandes datasets  (14M-300M imÃ¡genes) supera a resultados obtenidos previamente con modelos basados en CNNs.
:::

:::: {.columns}

::: {.column width="45%"}
[<img src="imagenes/papers/Dosovitskiy_etal_2021_title.png" width="90%"/>]{.center} 
:::

::: {.column width="45%"}
[<img src="imagenes/papers/Dosovitskiy_etal_2021_citing.png" width="100%"/>]{.center} 
:::

::::

::: {.center}

![](imagenes/papers/Dosovitskiy_etal_2021_fig_01.png){width="70%"}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.incremental .medium-par}
- **QuÃ© diferencia al lenguage de la visiÃ³n computacional?**
  - Densidad de informaciÃ³n: entrenar un modelo para predecir sÃ³lo pocas palabras faltantes parece inducir un entendimiento sofisticado del lenguaje (?).
  - En cambio, **imÃ¡genes tienen alta redundancia espacial**: segmentos faltantes pueden reconstruirse desde segmentos vecinos sin necesidad de entendimiento global.
  - SoluciÃ³n: **enmascarar una gran porciÃ³n de segmentos aleatorios, forzando entendimiento holÃ­stico de la imagen.**
:::

. . .

::: {.center}

![](imagenes/papers/He_etal_2022_title.png){width="50%"}

![](imagenes/papers/He_etal_2022_citing.png){width="60%"}

:::


## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.incremental .medium-par}
- AsÃ­ surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual:
  - Encoder + decoder. 
:::

. . .

::: {.center}

![](imagenes/papers/mae/mae_00.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.medium-par}
- AsÃ­ surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 1. Se divide input data en segmentos (*patches*), anÃ¡logos a a tokens en NLP.
:::

::: {.center}

![](imagenes/papers/mae/mae_01.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.medium-par}
- AsÃ­ surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 2. Segmentos se projectan linealmente a un espacio de embeddings, removiendo aleatoriamente una porciÃ³n significativa de tokens (ej: 75%).
:::

::: {.center}

![](imagenes/papers/mae/mae_02.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.medium-par}
- AsÃ­ surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 3. Tokens no removidos son pasados a travÃ©s del encoder.
  - **Encoder es un ViT, pero aplicado sÃ³lo a tokens no enmascarados.** 
:::

::: {.center}

![](imagenes/papers/mae/mae_03.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.medium-par}
- AsÃ­ surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 4. Token removidos son enmascarados, para indicar al modelo quÃ© fue removido del input.
:::

::: {.center}

![](imagenes/papers/mae/mae_04.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.medium-par}
- AsÃ­ surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- 5. Conjunto completo de tokens es pasado a travÃ©s del decoder, que intenta reconstruir el input.
:::

::: {.center}

![](imagenes/papers/mae/mae_05.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::



## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.medium-par}
- AsÃ­ surge un simple, efectivo y escalable *masked autoencoder* (MAE) para aprendizaje visual.
- **En este proceso se entrena encoder y decoder, pero sÃ³lo se mantiene el encoder para hacer inferencias en tareas *downstream*.**
:::

::: {.center}

![](imagenes/papers/mae/mae_06.png){width="50%"}

[(He et al., 2022)]{.center .small-par}

:::




## [Lenguaje ğŸ” VisiÃ³n Computacional]{.big-par}

::: {.incremental .medium-par}
- Ejemplos en datasets ImageNet y COCO.
  - Imagen enmascarada (izquierda), reconstrucciÃ³n con MAE (centro) y original (derecha).
:::

. . .

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/He_etal_2022_fig_02.png){.fragment .fade-in-then-out width="80%"}

![](imagenes/papers/He_etal_2022_fig_03.png){.fragment .fade-in-then-out width="80%"}

:::

[(He et al., 2022)]{.center .small-par}



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales]{.big-par}

::: {.incremental .medium-par}
- **P**retrained **R**emote **S**ensing **T**ransf**o**rmer: **Presto**.
:::

. . .

::: {.center}

![](imagenes/papers/papers_arrow/papers_arrow.png){width="90%"}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Modelo basado en *self-supervised masked autoencoding*: encoder-decoder (He et al., 2022).
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_00.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Permite ingestar series de tiempo nivel de pixel desde mÃºltiple productos y sensores de percepciÃ³n remota.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_01.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::


## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Series de tiempo son enmascaradas aleatoriamente y modelo es entrenado para reconstruir las series originales.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_02.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Encoder (f) embebe la parte del input no enmascarada.
:::

::: {.center}
![](imagenes/papers/presto_mae/presto_mae_03.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- Decoder (g) intenta reconstruir la parte enmascarada.
:::

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/presto_mae/presto_mae_04.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_fig_02.png){.fragment .fade-in-then-out width="80%"}

:::

[(Tseng et al., 2024)]{.center .small-par}



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- Una vez entrenado, sÃ³lo se usa output del encoder.
  - **Encoder puede usarse como *feature extractor* o en *fine-tuning*.**
  - Las representaciones aprendidas pueden ser adaptadas a una amplia variedad de tareas, con **diferentes sensores como input y con diverso nÃºmero de *timesteps***.
  <!-- - Predicciones son robustas a *missing data*... -->
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Datos de entrenamiento:** dataset representativo globalmente (21.5M pixel samples, resoluciÃ³n de 10m por pixel).
  - Series de tiempo a nivel de pixel para 12 meses (sampleados desde 2 aÃ±os en 2020-2021); 1 timestep por mes.
  - MÃºltiples set exportados desde Google Earth Engine.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_fig_06.png){width="60%"}
![](imagenes/papers/Tseng_etal_2024_datasets.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Encoding y tokenizaciÃ³n:**
  - **Series de tiempo son transformadas en tokens, c/u representado por un embedding.**
    - Por cada timestamp, se agrupan variables del input segÃºn tipo de sensor, resoluciÃ³n espacial y parte del espectro electromagnÃ©tico (ej: bandas S1 forman un grupo).
    - Cada token es una transformaciÃ³n lineal de un grupo de canales del input. 
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_channel_groups.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Encoding y tokenizaciÃ³n:**
  - Se quiere comunicar al modelo caracterÃ­sticas propias de imÃ¡genes satelitales: **localizaciÃ³n de pixeles, timestamp, grupo de canales**.
  - Se agregan encodings a los embeddings, que contienen una concatenaciÃ³n de:
    - la ubicaciÃ³n de cada pixel en la imagen,
    - timestamp (mes),
    - grupo de canales de cada variable.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_encodings.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Enmascaramiento:** clave para la performance de Presto en inputs incompletos (timesteps y/o canales faltantes).
  - Al enmascarar **se incentiva que el modelo aprenda representaciones que funcionen con subconjuntos de bandas o timesteps.**
  - En cada instancia de entrenamiento, se usÃ³ un sampleo aleatorio basado en mÃºltiples estrategias de enmascaramiento:
    - Aleatorio (1), canales-grupos (2), timesteps contiguos (3), timesteps no contiguos (4).
:::

<!-- . . . -->

::: {.center}
![](imagenes/papers/Tseng_etal_2024_masking_techniques.png){width="50%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Experimentos:** a partir del encoder-decoder entrenado, **se descartÃ³ el decoder y se pasÃ³ el output del encoder a un modelo downstream.**
  - Se evaluaron diferentes modelos para: 
    - **Feature extraction:** regresiÃ³n lineal o logÃ­stica (PrestoR), random forest (PrestoRF).
    - **Fine-tuning:** PrestoFT.
  - En diferentes tareas, basadas en **series de tiempo, imÃ¡genes, imÃ¡genes + series de tiempo**.
:::

. . .

::: {.center}
![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_00.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en series de tiempo**:
  - **Crop type segmentation:** clasificaciÃ³n de pixel binaria de: <br> maÃ­z (Kenya), cafÃ© (Brazil), terreno de cultivo (Togo).
:::

<!-- . . . -->

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_01.png){.fragment .fade-in-then-out width="65%"}

![](imagenes/papers/Tseng_etal_2024_table_02.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_fig_03.png){.fragment .fade-in-then-out width="80%"}

:::

[(Tseng et al., 2024)]{.center .small-par}


## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en series de tiempo**: regresiÃ³n
  - **Fuel moisture:** nivel de humedad del combustible vivo (contenido de agua en vegetaciÃ³n) en EEUU (Oeste).
  - **Algae blooms:** severidad de florecimiento de cianobacterias (algas verde-azuladas) en diferentes partes de EEUU.
:::

<!-- . . . -->

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_02.png){.fragment .fade-in-then-out width="65%"}

![](imagenes/papers/Tseng_etal_2024_table_03.png){.fragment .fade-in-then-out width="70%"}

:::

[(Tseng et al., 2024)]{.center .small-par}



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en imÃ¡genes**:
  - **TreeSatAI:** The TreeSatAI dataset consists of detecting the presence of one or more tree species (out of 20 possible species) in forestry images in Germany.
  - **EuroSAT:** The EuroSAT dataset classifies Sentinel-2 multispectral images in Europe with one of 10 landcover classes.
:::

<!-- . . . -->

::: {.r-stack fragment-index=2 .center}

![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_03.png){.fragment .fade-in-then-out width="65%"}

![](imagenes/papers/Tseng_etal_2024_fig_04.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_table_04.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_fig_05.png){.fragment .fade-in-then-out width="70%"}

![](imagenes/papers/Tseng_etal_2024_table_05.png){.fragment .fade-in-then-out width="70%"}

:::

[(Tseng et al., 2024)]{.center .small-par}



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.medium-par}
- **Reproducibilidad**
  - [Github repo](https://github.com/nasaharvest/presto) + [Jupyter notebook](https://github.com/nasaharvest/presto/blob/main/downstream_task_demo.ipynb): ejemplo de cÃ³mo aplicar Presto a nueva tarea.
  - [PresentaciÃ³n youtube](https://www.youtube.com/watch?v=H2mHNaI3oNQ)
:::

::: {.center}
![](imagenes/papers/presto_github_01.png){width="60%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Alternativas]{.big-par}

Presto no es el Ãºnico **Modelos Fundacional** propuestos para imÃ¡genes satelitales:

::: {.center}
![](imagenes/data/nasa_foundation_models.png){.fragment .fade-in-then-out width="85%"}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Alternativas]{.big-par}

Presto no es el Ãºnico **Modelos Fundacional** propuestos para imÃ¡genes satelitales:

::: {.center}
![](imagenes/papers/Rolf_etal_2021_fig_01.png){width="100%"}
[(A generalizable and accessible approach to machine learning with global satellite imagery. Rolf et al., 2021)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Alternativas]{.big-par}

Presto no es el Ãºnico **Modelos Fundacional** propuestos para imÃ¡genes satelitales:

::: {.center}
![](imagenes/papers/Rolf_etal_2021_fig_01.png){width="100%"}
[(A generalizable and accessible approach to machine learning with global satellite imagery. Rolf et al., 2021)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Alternativas]{.big-par}

::: {.center}
![](imagenes/papers/Jakubik_etal_2023_news.png){width="70%"}
![](imagenes/papers/Jakubik_etal_2023_fig_01.png){width="70%"}
![](imagenes/papers/Jakubik_etal_2023_fig_03.png){width="70%"}
[(Foundation Models for Generalist Geospatial Artificial Intelligence. Jakubik et al., 2023)]{.center .small-par}
:::



## [ImÃ¡genes satelitales â© INE (Futuro)]{.big-par}

::: {.r-stack fragment-index=2 .center}

![](imagenes/workflow/workflow_00.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01b.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_01c.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_02a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_03a.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_04.png){.fragment .fade-in-then-out width="90%"}

![](imagenes/workflow/workflow_05.png){.fragment .fade-in-then-out width="90%"}

:::



## [ConclusiÃ³n]{.big-par}

::: {.incremental .medium-par}
- Podemos aprovechar el potencial que tiene la **combinaciÃ³n de imÃ¡genes satelitales y machine learning (ML)** para enriquecer el cÃ¡lculo de indicadores estadÃ­sticos, superando los **desafÃ­os** planteados:
  - *El volumen existente de **datos satelitales etiquetados es mucho menor que de datos no-etiquetados**.*
    - Podemos utilizar modelos fundacionales (FM) pre-entrenados (Presto, etc), ya sea para fine-tuning o feature extraction.
    - Estos modelos capturan el comportamiento emergente del sistema modelado a partir de datos no-etiquetados y tienden a escalar mejor para tareas downstream que modelos especÃ­ficos entrenados con datos etiquetados.
  - *En general, por diseÃ±o, **resultados de encuestas son limitados geogrÃ¡fica y temporalmente**.*
    - Predicciones obtenidas desde el fine-tuning o feature extraction de FMs pueden generalizar estos resultados, especialmente a regiones o intervalos de tiempo en que son escasos.
  - *Existe **limitada infraestructura y capacidad tÃ©cnica** para que analistas entrenen modelos ML que usen datos de imÃ¡genes satelitales, personalizados para cada tarea a resolver.*
    - El fine-tuning o feature extraction de FMs permite entrenar pequeÃ±os modelos, con datos etiquetados relevantes para cada tarea.
    - Este entrenamiento no requiere una gran infraestructura ni datos satelitales, por lo que eventualmente podrÃ­a ser realizado por analistas.
      - Ejemplo: calcular predicciones de algÃºn indicador a un dado nivel geogrÃ¡fico, que sirva de insumo para modelos basados en *Small Are Estimation* (SAE).
:::



## [Extra]{.big-par}



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en imÃ¡genes + series de tiempo**:
  - **S2-Agri100:** El dataset S2-Agri clasifica tipos de cultivo crop en parcelas agrÃ­colas.
  - Consiste de 24 timesteps en intervalos de 10-30 dÃ­as (comparado con Presto, que fue entrenado en series de tiempo de 12 meses).
:::

. . .

::: {.center}
![](imagenes/papers/presto_downstream_tasks/presto_downstream_task_04.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [VisiÃ³n Computacional ğŸ” ImÃ¡genes Satelitales | Presto]{.big-par}

::: {.incremental .medium-par}
- **Tareas basadas en imÃ¡genes + series de tiempo**:
  - **S2-Agri100:** El dataset S2-Agri clasifica tipos de cultivo crop en parcelas agrÃ­colas. 
  - Presto logra resultados comparables a modelo SITS-Former, a pesar de tener 6x menos parÃ¡metros.
    - Esto demuestra que Presto puede recibir series de tiempo con variada resoluciÃ³n y a diferentes intervalos temporales.
:::

. . .

::: {.center}
![](imagenes/papers/Tseng_etal_2024_table_06.png){width="70%"}
[(Tseng et al., 2024)]{.center .small-par}
:::



## [ImÃ¡genes satelitales â© INE (Futuro)]{.big-par}

```{mermaid}
flowchart TD
    L11[Dagster: GEE] & L12[Etiquetas tarea: SAE, MMV, etc] --> L2[Presto: encoder]
    L2 --> L3[Dependencia local o regional?]
    L3 -- local ---> L41[Fine Tuning]
    L41 --> L51[Ej: SAE]
    L3 -- regional ---> L42[Feature extraction]
    L42 --> L52[Ej: MMV]
```


#

[<img src="imagenes/logo_portada2.png" width="20%"/>]{.center}

[**ImÃ¡genes Satelitales**]{.big-par .center-justified}
[***Para* Ciencia de Datos**]{.medium-par.center-justified}

[**Agosto 2024**]{.big-par .center-justified}


[]{.linea-superior} 
[]{.linea-inferior} 



